--- 
title: "Finding $$k$$ for $$k$$-means clustering"
author: "Arjun Poddar"
date: "January 01, 2016"
---

## Introduction

<a href="https://en.wikipedia.org/wiki/K-means_clustering" target ="_blank"> k-means clustering</a> is a very popular method to find clusters in a set of data. 

Suppose we have a set of $$n$$ data points(observations) $$(x_1, x_2, \dots, x_n)$$, where each $$x_i$$ is a point in a $$d$$-dimensional space. This means that $$x_i$$ is the vector $$(x_{i1}, x_{i2}, \dots, x_{id})$$, for $$i = 1, 2, \dots, n$$. 


In k-means clustering these $$n$$ data points or vectors are devided or grouped into $$k$$ groups or "clusters" based on how similar the data points are measured by the $$d$$-dimensions. Generally, this "similarity" between the points are calculated by using distance functions, for example the <a href= "https://en.wikipedia.org/wiki/Euclidean_distance" target="_blank"> Euclidean distance</a> .




When the value of $$k$$ is known, here is, in a nutshell, how the k-means clustering algorithm works: 


* Assignment: Each of the $$n$$ points of data are assigned to any of the $$k$$ groups in such a way that the **within-cluster sum of squares** (WCSS) is minimized. 

* Update: The centers for all the clusters are updated using the points in allocated to them.




In <a href="https://www.r-project.org/" target ="_blank"> R</a>, the function  <a
href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html" target="_blank"> kmeans</a> can be used to perform k-means clustering on a data set. The two primary arguements of this function are namely $$x$$ and centers, where $$x$$ is the numeric data in appropriate form (points or observations arranged in rows and the different dimensions of the observations in columns) and centers is k(the number of clusters) or a set of initial seed for the distinct $$k$$ centers.


## Finding the right k

If the number of clusters is known, $$k$$-means clustering is easy to do. But more often than not, $$k$$ is unknown because the distribution of the data may be unknown. If $$k$$ is too small with respect to the size of the data, clustering may not capture the true heterogeneity present in the data. On the other hand, if $$k$$ is too large a number, using the clustering information may not be useful at all. So, we have to find a balance.

TSS = BCSS  + WCSS


If we change $$k$$, WCSS changes(for obvious reasons) but TSS does not. Let us verify this with a simple R code. We will use the numeric columns from the famous <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set" target="_blank"> Iris</a> data set.


```r
# set the data
df <- iris[, 1:4]
#check the data
head(df)
```

```
##   Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1          5.1         3.5          1.4         0.2
## 2          4.9         3.0          1.4         0.2
## 3          4.7         3.2          1.3         0.2
## 4          4.6         3.1          1.5         0.2
## 5          5.0         3.6          1.4         0.2
## 6          5.4         3.9          1.7         0.4
```

```r
# k-means clustering with k = 7
iris.cluster.7 <- kmeans(df, 7)
# k-means clustering with k = 11
iris.cluster.11 <- kmeans(df, 11)

#check the TSSs from the two clusterings 
cbind(iris.cluster.7$$totss, iris.cluster.11$$totss)
```

```
##          [,1]     [,2]
## [1,] 681.3706 681.3706
```

```r
#check the WCSSs from the two clusterings 
cbind(iris.cluster.7$$tot.withinss, iris.cluster.11$$tot.withinss)
```

```
##          [,1]     [,2]
## [1,] 34.83092 25.56388
```

```r
#check the BCSSs from the two clusterings 
cbind(iris.cluster.7$$betweenss, iris.cluster.11$$betweenss)
```

```
##          [,1]     [,2]
## [1,] 646.5397 655.8067
```
From the last three outputs, it is clear that though the TSS is constant regardless of what $$k$$ we choose, WCSS and BCSS are not. For each k-means clustering WCSS is calculated byt calculating the sums of squares of distance of each point from the center of the cluster it belongs to. BCSS is obtained from subtracting WCSS from TSS.

A smaller WCSS implies a better clustering of the data points. So to choose a $$k$$ that gives us better result, we might look for a smaller WCSS or a bigger BCSS. 
As a criteria to choose the best $$k$$, we will run the k-means clustering for different values of $$k$$ and choose the one which has the highest BCCS and we will plot the BCSS/TSS. We choose BCSS instead of WCSS because BCSS/TSS reminds us of $$R^2$$, <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination" target="_blank"> coeffieint of determination</a>. We call this ratio "variance explained" by the clustering.

So we write a function in R named cluster.k which takes three arguements - df(the data frame), k.min (minimum value of k) and k.max(maximum value of k). The default value for k.min is 2 and for k.max is $$ceiling(\sqrt{(n/2)})$$.

When we run this function on a dataset, a plot of percentage of variance explained and k is produced along with a datafrane named variance.explained and a numeric value called k for which the variance is maximized.



```r
cluster.k <- function(data, kmin, kmax){
  d.matrix <- data.matrix(data, rownames.force  =  NA)
  variance.explained <- c()
  
  for(i in kmin: kmax){
    q <- try(kmeans(d.matrix, i), TRUE)
    
    if (class(q) != "try-error"){
      variance.explained <- c(variance.explained, round(q$$betweenss/q$$totss, 6)*100)
    } else{
      variance.explained <- c(variance.explained, -1)
    }
  }
  cat("Percentage of Variance Explained \n")
  print(variance.explained)
  cat("\n")
  k <- which.max(variance.explained) + (kmin - 1)
  print(paste0("The variance explained is highest for k=", k))
  plot(x = seq(kmin, kmax), y = variance.explained, type = "b", 
       xlab = "Number of Clusters", ylab = " % of Variance Explained")
}
 
cluster.k(iris[,1:4], 5, 20)
```

```
## Percentage of Variance Explained 
##  [1] 92.6418 93.8793 94.8990 94.7189 95.9220 95.1512 96.4467 96.3783
##  [9] 96.7231 96.7219 96.6712 97.2137 97.2103 97.1417 97.3113 97.4017
## 
## [1] "The variance explained is highest for k=20"
```

![plot of chunk unnamed-chunk-2](figure/unnamed-chunk-2-1.png)
